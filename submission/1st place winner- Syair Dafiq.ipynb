{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.evaluate import GroupTimeSeriesSplit\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,PowerTransformer,OrdinalEncoder,StandardScaler,MinMaxScaler,PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer,TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer  \n",
    "\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.count import CountEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,ElasticNet\n",
    "from sklearn.ensemble import StackingRegressor,VotingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,TimeSeriesSplit,train_test_split,cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of my Solutions\n",
    "\n",
    "<ul>\n",
    "    <li> For time related feature (CPI,rent index,etc) im using lagged value in 3 periods, for example if i want predict price in month 12, i use the time related feature in month 9. i used 3 month since the problem is to predict 3 months ahead. </li>\n",
    "    <li> i used feature engineering in general, like mean of column, diff, and etc. u can see below in feature engineering section. </li>\n",
    "    <li> i used tranformation for my numeric variables, for categorical variables i used target encoding or mean encoding\n",
    "    </li>\n",
    "    <li> feature selection using rfecv (recursive feature elimination with cross validation) </li>\n",
    "    <li> for CV method i use time series grouped Cross validation i used statsmodels library for this method, since there are multiple house in each days. </li>\n",
    "    <li> im also done with hyperparameter tune with each models and different subset of features </li>\n",
    "    <li> my final model is stacking from 9 different models, 3 LightGBM, 3 Catboost, 3 XGBoost, each models has different features and different parameters. and i used Standard LightGBM with no tune for final estimators </li>\n",
    "</ul> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi = pd.read_csv(\"../datasets/raw/cpi.csv\",parse_dates=['Data Series'])\n",
    "geo_attr = pd.read_csv(\"../datasets/raw/geo_attributes.csv\")\n",
    "interest = pd.read_csv(\"../datasets/raw/interest.csv\",parse_dates=['Data Series'])\n",
    "properties = pd.read_csv(\"../datasets/raw/properties.csv\")\n",
    "rentindex = pd.read_csv(\"../datasets/raw/rentindex.csv\",parse_dates=['Data Series'])\n",
    "test = pd.read_csv(\"../datasets/raw/test.csv\",parse_dates=['contractDate'])\n",
    "train = pd.read_csv(\"../datasets/raw/train.csv\",parse_dates=['contractDate'])\n",
    "vacant = pd.read_csv(\"../datasets/processed/iteration_1/vacant_edit.csv\",parse_dates=['Data Series'])   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the date format in rent index is different with others, so i try to change it to the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_edit = rentindex.copy()\n",
    "ri_edit['Data Series'] = ri_edit['Data Series'].apply(lambda x: x.replace(' 1Q','-01-01'))\n",
    "ri_edit['Data Series'] = ri_edit['Data Series'].apply(lambda x: x.replace(' 2Q','-04-01'))\n",
    "ri_edit['Data Series'] = ri_edit['Data Series'].apply(lambda x: x.replace(' 3Q','-07-01'))\n",
    "ri_edit['Data Series'] = ri_edit['Data Series'].apply(lambda x: x.replace(' 4Q','-10-01')) \n",
    "ri_edit['Data Series'] = pd.to_datetime(ri_edit['Data Series'])\n",
    "#resampling 3 times each month fill with value in 3 period before\n",
    "ri_edit = ri_edit.set_index('Data Series').resample('M').ffill().reset_index()\n",
    "#add 2022-11-10, fill with 148.1 \n",
    "ri_edit = pd.concat([ri_edit,pd.DataFrame({'Data Series':pd.to_datetime('2022-11-10'),'RentIndex':148.1},index=[0])]).sort_index().reset_index(drop=True)\n",
    "#add 2022-12-10, fill with 148.1\n",
    "ri_edit = pd.concat([ri_edit,pd.DataFrame({'Data Series':pd.to_datetime('2022-12-10'),'RentIndex':148.1},index=[0])]).sort_index().reset_index(drop=True)\n",
    "\n",
    "#replcae day with 01 \n",
    "ri_edit['Data Series'] = ri_edit['Data Series'].apply(lambda x: x.replace(day=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the merge datasets on train and submission datasets with lagged of time related feature (rentindex, cpi, interest, vacant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_merge(df_tmp,df1,df2,df3,df4,df5): \n",
    "    data = pd.merge(df_tmp,df1,how='left',on='property_key')\n",
    "    data = pd.merge(data,geo_attr,how='left',on=['street','project','district'] )\n",
    "    data.rename(columns={'contractDate':'Data Series'},inplace=True)\n",
    "    \n",
    "    df_date = pd.merge(df2,df3,how='left',on='Data Series')\n",
    "    df_date = pd.merge(df_date,df4,how='left',on='Data Series')\n",
    "    df_date = pd.merge(df_date,df5,how='left',on='Data Series')\n",
    "    df_date['Data Series'] =  df_date['Data Series'].apply(lambda x: x + pd.DateOffset(months=3))\n",
    "\n",
    "    data = pd.merge(data,df_date,how='left',on='Data Series')\n",
    "    data.columns = data.columns.str.replace(' ','') \n",
    "    data.columns = data.columns.str.lower()\n",
    "    data = data[data[\"dataseries\"] > '2018-03-01']\n",
    "    return data \n",
    "\n",
    "df = function_merge(train,properties,cpi,vacant,interest,ri_edit)\n",
    "df_test = function_merge(test,properties,cpi,vacant,interest,ri_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_key</th>\n",
       "      <th>dataseries</th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>floorrange</th>\n",
       "      <th>propertytype</th>\n",
       "      <th>district</th>\n",
       "      <th>typeofarea</th>\n",
       "      <th>tenure</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>num_schools_1km</th>\n",
       "      <th>num_supermarkets_500m</th>\n",
       "      <th>num_mrt_stations_500m</th>\n",
       "      <th>cpi</th>\n",
       "      <th>available</th>\n",
       "      <th>vacant</th>\n",
       "      <th>interestrate</th>\n",
       "      <th>rentindex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>p-89d6ffc3d</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>1400000.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>01-05</td>\n",
       "      <td>Condominium</td>\n",
       "      <td>5</td>\n",
       "      <td>Strata</td>\n",
       "      <td>99 yrs lease commencing from 2004</td>\n",
       "      <td>WEST COAST ROAD</td>\n",
       "      <td>...</td>\n",
       "      <td>1.294645</td>\n",
       "      <td>103.767168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.035</td>\n",
       "      <td>30153.0</td>\n",
       "      <td>3950.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>102.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>p-7529081f2</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>738000.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>01-05</td>\n",
       "      <td>Condominium</td>\n",
       "      <td>16</td>\n",
       "      <td>Strata</td>\n",
       "      <td>99 yrs lease commencing from 2011</td>\n",
       "      <td>BEDOK RESERVOIR ROAD</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337536</td>\n",
       "      <td>103.920652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.035</td>\n",
       "      <td>30153.0</td>\n",
       "      <td>3950.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>102.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>p-b21ddfd36</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>3132500.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>21-25</td>\n",
       "      <td>Condominium</td>\n",
       "      <td>3</td>\n",
       "      <td>Strata</td>\n",
       "      <td>99 yrs lease commencing from 2012</td>\n",
       "      <td>PRINCE CHARLES CRESCENT</td>\n",
       "      <td>...</td>\n",
       "      <td>1.292695</td>\n",
       "      <td>103.820411</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.035</td>\n",
       "      <td>30153.0</td>\n",
       "      <td>3950.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>102.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>p-b1ace2a03</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>4500000.0</td>\n",
       "      <td>307.3</td>\n",
       "      <td>-</td>\n",
       "      <td>Semi-detached</td>\n",
       "      <td>20</td>\n",
       "      <td>Land</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>WESTLAKE AVENUE</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345626</td>\n",
       "      <td>103.836933</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99.035</td>\n",
       "      <td>30153.0</td>\n",
       "      <td>3950.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>102.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>p-0c92e94b9</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>1456000.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>06-10</td>\n",
       "      <td>Condominium</td>\n",
       "      <td>19</td>\n",
       "      <td>Strata</td>\n",
       "      <td>99 yrs lease commencing from 2007</td>\n",
       "      <td>HOUGANG STREET 11</td>\n",
       "      <td>...</td>\n",
       "      <td>1.351390</td>\n",
       "      <td>103.881242</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99.035</td>\n",
       "      <td>30153.0</td>\n",
       "      <td>3950.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>102.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    property_key dataseries      price   area floorrange   propertytype  \\\n",
       "242  p-89d6ffc3d 2018-04-01  1400000.0  122.0      01-05    Condominium   \n",
       "243  p-7529081f2 2018-04-01   738000.0   62.0      01-05    Condominium   \n",
       "244  p-b21ddfd36 2018-04-01  3132500.0  154.0      21-25    Condominium   \n",
       "245  p-b1ace2a03 2018-04-01  4500000.0  307.3          -  Semi-detached   \n",
       "246  p-0c92e94b9 2018-04-01  1456000.0  123.0      06-10    Condominium   \n",
       "\n",
       "     district typeofarea                             tenure  \\\n",
       "242         5     Strata  99 yrs lease commencing from 2004   \n",
       "243        16     Strata  99 yrs lease commencing from 2011   \n",
       "244         3     Strata  99 yrs lease commencing from 2012   \n",
       "245        20       Land                           Freehold   \n",
       "246        19     Strata  99 yrs lease commencing from 2007   \n",
       "\n",
       "                      street  ...       lat         lng  num_schools_1km  \\\n",
       "242          WEST COAST ROAD  ...  1.294645  103.767168              0.0   \n",
       "243     BEDOK RESERVOIR ROAD  ...  1.337536  103.920652              1.0   \n",
       "244  PRINCE CHARLES CRESCENT  ...  1.292695  103.820411              2.0   \n",
       "245          WESTLAKE AVENUE  ...  1.345626  103.836933             12.0   \n",
       "246        HOUGANG STREET 11  ...  1.351390  103.881242              6.0   \n",
       "\n",
       "     num_supermarkets_500m  num_mrt_stations_500m     cpi  available  vacant  \\\n",
       "242                    0.0                    0.0  99.035    30153.0  3950.0   \n",
       "243                    2.0                    1.0  99.035    30153.0  3950.0   \n",
       "244                    0.0                    1.0  99.035    30153.0  3950.0   \n",
       "245                    0.0                    3.0  99.035    30153.0  3950.0   \n",
       "246                    1.0                    3.0  99.035    30153.0  3950.0   \n",
       "\n",
       "     interestrate  rentindex  \n",
       "242           0.5      102.8  \n",
       "243           0.5      102.8  \n",
       "244           0.5      102.8  \n",
       "245           0.5      102.8  \n",
       "246           0.5      102.8  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "for data preparation i used train test split for time series, which mean i just use the test set from 2012-10-01 until 2022-12-01 so is 3 month periods in test set. The rest of data is used for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59761, 21) (2946, 21) (59761,) (2946,)\n"
     ]
    }
   ],
   "source": [
    "X = df.sort_values(by = 'dataseries' ).drop(columns = 'price')\n",
    "y = df.sort_values(by = 'dataseries' ).price\n",
    "\n",
    "X_train = X[X.dataseries < '2022-10-01']\n",
    "X_test = X[X.dataseries >= '2022-10-01']\n",
    "\n",
    "y_train = y[X.dataseries < '2022-10-01']\n",
    "y_test = y[X.dataseries >= '2022-10-01']\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Feature Engineering\n",
    "\n",
    "for feature engineering i just applied general feature engineering, which mean it can also applied on the other datasets too, some of the feature engineering also referrence from other discussion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make custom transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MakeDate(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['dataseries'] = pd.to_datetime(X_['dataseries'])\n",
    "        X_['year'] = X_['dataseries'].dt.year\n",
    "        X_['month'] = X_['dataseries'].dt.month\n",
    "        X_['quarter'] = X_['dataseries'].dt.quarter\n",
    "        X_['longday'] = (X_['dataseries'] - pd.to_datetime('2018-01-01')).dt.days\n",
    "        return X_\n",
    "    \n",
    "class MakeFloor(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['floor'] = X_['floorrange'].apply(lambda x: x.split('-')[1])\n",
    "        X_['floor'] = X_['floor'].replace({'':\"1\",'B5':\"-5\"} )\n",
    "        X_['floor'] = X_['floor'].astype(int)\n",
    "        X_[\"floor_area\"] = X_['floor'] * X_['area']\n",
    "        return X_\n",
    "    \n",
    "class MakeTenure(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['tenure2'] = X_['tenure'].str.split(\" \").str[-1]\n",
    "        X_['tenure2'] = X_['tenure2'].replace({'Freehold':-999})\n",
    "        X_['tenure2'] = X_['tenure2'].astype(int)\n",
    "\n",
    "        X_['tenure3'] = X_['tenure2'] - X_['year']\n",
    "        X_['tenure3'] = np.where(X_['tenure3'] <= -999,-999,X_['tenure3'])\n",
    "        \n",
    "        X_['tenure'] = X_['tenure'].str.split(\" \").str[0]\n",
    "        X_['tenure'] = np.where(X_['tenure'] == \"Freehold\",\"Freehold\",\"Not Freehold\")\n",
    "        return X_ \n",
    "class SumBuilding(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['sum_building'] = X_[['num_schools_1km','num_supermarkets_500m','num_mrt_stations_500m']].sum(axis=1)\n",
    "        X_['mean_building'] = X_[['num_schools_1km','num_supermarkets_500m','num_mrt_stations_500m']].mean(axis=1)\n",
    "        X_['std_building'] = X_[['num_schools_1km','num_supermarkets_500m','num_mrt_stations_500m']].std(axis=1)\n",
    "        return X_\n",
    " \n",
    "class TypeMarket(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['type_market'] = X_['typeofarea'] + \"_\" + X_['marketsegment']\n",
    "        X_['tenure_type'] = X_['tenure'] + \"_\" + ['typeofarea']\n",
    "        X_['tenure_market'] = X_['tenure'] + \"_\" + ['marketsegment']\n",
    "        return X_\n",
    "class MakeStreet(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.street_data = {\n",
    "            'street_type': [\n",
    "                'ROAD', 'AVENUE', 'DRIVE', 'STREET', 'LORONG', 'WALK', 'CRESCENT','LINK', 'RISE', 'LANE', 'TERRACE', 'BOULEVARD', 'CIRCLE', 'LOOP','COMMONWEALTH', 'QUAY'\n",
    "            ],\n",
    "            'geo_type': [\n",
    "                'PASIR', 'BAY', 'KEPPLE', 'TANAH', 'COAST', 'BEACH', 'SELETAR','SUNRISE', 'TAI', 'COVE'\n",
    "            ],\n",
    "            'wind_dir': ['EAST', 'WEST', 'NORTH', 'SOUTH']\n",
    "        }\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        for col in self.street_data:\n",
    "            X_[col.lower()] = X_['street'].apply(lambda x: next((i for i in self.street_data[col] if i in x.upper()), 'OTHER'))\n",
    "        return X_\n",
    "class DistrictGroup(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.district_month = None\n",
    "    def fit(self,X,y):\n",
    "        X_ = X.copy() \n",
    "        X_['price'] = y\n",
    "        self.district_month_dict = X_.groupby(['district', 'month'])['price'].mean().to_dict()\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['district_month'] = X_.apply(lambda x: self.district_month_dict[(x['district'], x['month'])], axis=1)\n",
    "        X_['district_area'] = X_.groupby('district')['area'].transform('mean')\n",
    "        return X_\n",
    "\n",
    "class DistrictGroup(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['district_area'] = X_.groupby('district')['area'].transform('mean')\n",
    "        return X_\n",
    "\n",
    "\n",
    "pipeline_feat = Pipeline([\n",
    "    ('make_date',MakeDate()),\n",
    "    ('make_floor',MakeFloor()),\n",
    "    ('make_tenure',MakeTenure()),\n",
    "    ('sum_building',SumBuilding()),\n",
    "    ('type_market',TypeMarket()),\n",
    "    ('make_street',MakeStreet()),\n",
    "    ('DistrictGroup',DistrictGroup())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing that i used for numerical data is standard scaler and yeo-johnson transformation and for categorical data i used target encoder and count encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col =  ['floor','area','lat','lng','num_schools_1km','num_supermarkets_500m','num_mrt_stations_500m',\n",
    "            'cpi','vacant','interestrate','rentindex','available','sum_building','mean_building','std_building',\n",
    "            'floor_area','district_area']\n",
    "cat_col = ['propertytype','district','typeofarea','marketsegment',\"tenure\",\"type_market\",\"street_type\",\"geo_type\",\n",
    "           \"wind_dir\",\"tenure_type\",\"tenure_market\"]\n",
    "freq_col = ['street','project']\n",
    "notrans_col = ['year','month','quarter','longday','tenure2','tenure3']\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    ('sscale',StandardScaler()),\n",
    "    ('power',PowerTransformer())\n",
    "])\n",
    "\n",
    "notrans_pipe = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "te_pipe = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot',TargetEncoder(cat_col))\n",
    "]) \n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_pipe',num_pipe,num_col),\n",
    "    ('cat_pipe',te_pipe,cat_col),\n",
    "    ('notrans_pipe',notrans_pipe,notrans_col)\n",
    "],verbose_feature_names_out= False).set_output(transform = \"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for model training and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training\n",
    "def train_model_ts(list_model,X_train,y_train,X_test,y_test,metric,cv,scorer,pipeline,groups):\n",
    "    df_model = pd.DataFrame(columns = [\"model_name\",\"set_data\",\"score\",\"model\"])\n",
    "    set_data = [\"test\",\"cv\",\"train\"]\n",
    "\n",
    "    for m in list_model: \n",
    "        pipeline_copy = deepcopy(pipeline)\n",
    "        pipeline_copy.set_params(model = list_model[m])\n",
    "        spot_check = cross_val_score(pipeline_copy,X_train,y_train,cv = cv,scoring = scorer,n_jobs= -1,groups = groups )\n",
    "        spot_check = spot_check.mean()\n",
    "        model = pipeline_copy.fit(X_train,y_train)\n",
    "        score = metric(y_test,model.predict(X_test),squared = False)\n",
    "        score_train = metric(y_train,model.predict(X_train),squared = False)\n",
    "        model_list = [m] * 3\n",
    "        tes = pd.DataFrame(list(zip(model_list,set_data,[score,spot_check,score_train],[model,model,model])),columns = [\"model_name\",\"set_data\",\"score\",\"model\"])\n",
    "        print(f\"model {m} selesai di training\")\n",
    "        print(f\"score test {score}\")\n",
    "        print(f\"score cv {spot_check}\")\n",
    "        print(f\"score train {score_train}\")\n",
    "        print(\"=====================================\")\n",
    "        df_model = pd.concat([df_model,tes],ignore_index = True)\n",
    "        \n",
    "    return df_model\n",
    "\n",
    "\n",
    "#function for feature selection \n",
    "#since im using pipeline for my workflow, so ineed to modify the rfecv function from scikit-learn, \n",
    "# so it can work with pipeline. other than that is same as scikit-learn rfecv function\n",
    "def rfecv(X, y, pipeline,min_features_to_select=3, cv = 3,step=3,scoring_metric=\"f1\",scoring_decimals=3,random_state=42,groups = None):\n",
    "    # Initialize survivors and ranked list\n",
    "    estimator = deepcopy(pipeline)\n",
    "    estimator.steps.pop(-1)\n",
    "    survivors = estimator.fit_transform(X_train,y_train).columns.tolist()\n",
    "    ranks = []\n",
    "    scores = []\n",
    "    # While the survivor list is longer than min_features_to_select\n",
    "    while len(survivors) >= min_features_to_select:\n",
    "        print(ranks)\n",
    "        remove_column_transformer = FunctionTransformer(lambda x: x.drop(ranks, axis=1))\n",
    "        estimator = deepcopy(pipeline)\n",
    "        estimator.steps.insert(-1, ('remove_column_transformer', remove_column_transformer))\n",
    "        # Get only the surviving features\n",
    "        \n",
    "        # Train and get the scores, cross_validate clones \n",
    "        # the model internally, so this does not modify\n",
    "        # the estimator passed to this function\n",
    "        print(\"[%.2f] evaluating %i features ...\" % (time(), len(survivors)))\n",
    "        cv_result = cross_validate(estimator, X, y,\n",
    "                                cv=cv,\n",
    "                                groups = groups,\n",
    "                                scoring=scoring_metric,\n",
    "                                return_estimator=True)\n",
    "        # Append the mean performance to \n",
    "        score = np.mean(cv_result[\"test_score\"])\n",
    "        if scoring_decimals is None:\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            scores.append(round(score, scoring_decimals))            \n",
    "        print(\"[%.2f] ... score %f.\" % (time(), scores[-1]))\n",
    "        \n",
    "        # Get feature weights from the model fitted \n",
    "        # on the best fold and square the weights as described \n",
    "        # in the paper. If the estimator is a Pipeline,\n",
    "        # we get the weights from the last element.\n",
    "        best_estimator = cv_result[\"estimator\"][np.argmax(cv_result[\"test_score\"])]\n",
    "        if isinstance(best_estimator, Pipeline):\n",
    "            weights = best_estimator[-1].feature_importances_\n",
    "        else:\n",
    "            weights = best_estimator.feature_importances_\n",
    "        weights = list(np.power(weights, 2))\n",
    "                \n",
    "        # Remove step features (but respect min_features_to_select)\n",
    "        for _ in range(max(min(step, len(survivors) - min_features_to_select), 1)):\n",
    "            \n",
    "            # Find the feature with the smallest ranking criterion\n",
    "            # and update the ranks and survivors\n",
    "            idx = np.argmin(weights)\n",
    "            ranks.insert(0, survivors.pop(idx))\n",
    "            weights.pop(idx)\n",
    "            \n",
    "    # Calculate the best set of surviving features\n",
    "    ranks_reverse = list(reversed(ranks))\n",
    "    last_max_idx = len(scores) - np.argmax(list(reversed(scores))) - 1\n",
    "    removed_features = set(ranks_reverse[0:last_max_idx * step])\n",
    "    best_features = [f for f in X.columns if f not in removed_features]\n",
    "    \n",
    "    # Return ranks and scores\n",
    "    return best_features, max(scores), ranks, scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model CatB selesai di training\n",
      "score test 437245.36890725215\n",
      "score cv -443580.90784629434\n",
      "score train 262710.9501523442\n",
      "=====================================\n",
      "model LGBMRegressor selesai di training\n",
      "score test 428930.59875250614\n",
      "score cv -460846.58360564813\n",
      "score train 312696.87586684607\n",
      "=====================================\n",
      "model XGBRegressor selesai di training\n",
      "score test 421815.81806846306\n",
      "score cv -474717.2412766598\n",
      "score train 222358.22258086115\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "pipeline_base = Pipeline([\n",
    "    ('pipe_feat',pipeline_feat),\n",
    "    ('preprocessor',preprocessor),\n",
    "    ('model',None)\n",
    "])\n",
    "\n",
    "list_model = {\n",
    "   'CatB' : CatBoostRegressor(iterations = 300,silent = True),\n",
    "    'LGBMRegressor':LGBMRegressor(max_depth = -1),\n",
    "    'XGBRegressor':XGBRegressor(tree_method = 'hist'),\n",
    "}\n",
    "\n",
    "\n",
    "cv = GroupTimeSeriesSplit(n_splits = 5,test_size=5,gap_size = 3)\n",
    "groups = pd.factorize(X_train.dataseries)[0]\n",
    "\n",
    "df_model = train_model_ts(list_model,X_train,y_train,X_test,y_test,mean_squared_error,cv,\n",
    "                        \"neg_root_mean_squared_error\",pipeline_base,groups)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lightgbm\n",
    "pipeline_base = Pipeline([\n",
    "    ('pipe_feat',pipeline_feat),\n",
    "    ('preprocessor',preprocessor),\n",
    "    ('model',LGBMRegressor())\n",
    "])\n",
    "\n",
    "#run to get the best features\n",
    "#rfecv(X_train,y_train,pipeline_base,min_features_to_select=10,step = 1, cv = cv,scoring_metric=\"neg_root_mean_squared_error\",groups = groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xgboost \n",
    "pipeline_base = Pipeline([\n",
    "    ('pipe_feat',pipeline_feat),\n",
    "    ('preprocessor',preprocessor),\n",
    "    ('model',XGBRegressor(random_state = 42,n_jobs=-1,tree_method = \"hist\"))\n",
    "])\n",
    "\n",
    "rfecv(X_train,y_train,pipeline_base,min_features_to_select=15,step = 1, cv = cv,scoring_metric=\"neg_root_mean_squared_error\",groups = groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for catboost\n",
    "pipeline_base = Pipeline([\n",
    "    ('pipe_feat',pipeline_feat),\n",
    "    ('preprocessor',preprocessor),\n",
    "    ('model',CatBoostRegressor(iterations = 300,silent = True))\n",
    "])\n",
    "\n",
    "rfecv(X_train,y_train,pipeline_base,min_features_to_select=15,step = 1, cv = cv,scoring_metric=\"neg_root_mean_squared_error\",groups = groups)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the syntax from above for another model, i can get the best feature to use for my model. Im also explicitly stating the feature should i drop, so in the end i get the 2 part of the best feature for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_lgb = ['cpi', 'district_area', 'num_schools_1km', 'floor', 'rentindex', 'marketsegment', 'typeofarea', 'geo_type', 'month', 'vacant', 'tenure', 'interestrate', 'available', 'wind_dir', 'quarter', 'year', 'tenure_market', 'tenure_type', 'mean_building']\n",
    "drop_cols_xgb = ['interestrate', 'cpi', 'wind_dir', 'available', 'vacant', 'quarter', 'year', 'tenure_market', 'tenure_type', 'mean_building']\n",
    "drop_cols_catb = ['cpi', 'month', 'floor', 'year', 'geo_type', 'rentindex', 'vacant', 'quarter', 'wind_dir', 'interestrate', 'available', 'tenure_market', 'tenure_type', 'tenure']\n",
    "drop_cols_catb2= ['mean_building', 'geo_type', 'rentindex', 'available', 'month', 'vacant', 'interestrate', 'year', 'wind_dir', 'tenure_market',\n",
    "                  'quarter', 'tenure', 'tenure_type','district_area','street','project']\n",
    "drop_cols_xgb2 = ['month', 'geo_type', 'rentindex', 'interestrate', 'available', 'cpi', 'wind_dir', 'mean_building', 'vacant', 'quarter', 'year', \n",
    "                  'tenure_market', 'tenure_type','district_area','street','project']\n",
    "drop_cols_lgb2 = ['floor', 'rentindex', 'geo_type', 'month', 'typeofarea', 'tenure', 'interestrate', 'vacant', 'available', 'wind_dir', 'mean_building', \n",
    "                  'quarter', 'year', 'tenure_market', 'tenure_type','district_area','street','project']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "for final model i used stacking method with LightGBM for my final estimators, i have 9 total of models with consist of 3 baseline models, and 6 models that have been hyperparameter tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model stack1 selesai di training\n",
      "score test 396835.5255616923\n",
      "score cv -453974.0373185974\n",
      "score train 337876.6682145801\n",
      "=====================================\n",
      "model stack2 selesai di training\n",
      "score test 411736.64360886684\n",
      "score cv -469501.3772593538\n",
      "score train 337579.7132624123\n",
      "=====================================\n",
      "model voting selesai di training\n",
      "score test 377446.47130130866\n",
      "score cv -429777.7355599934\n",
      "score train 186252.99858357332\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "pipeline_catb = Pipeline([\n",
    "    ('drop_cols',FunctionTransformer(lambda x: x.drop(drop_cols_catb, axis=1))),\n",
    "    (\"algo\",CatBoostRegressor(iterations = 300,silent = True,colsample_bylevel = 0.9599,max_depth = 8 ))\n",
    "])\n",
    "\n",
    "pipeline_lgb = Pipeline([\n",
    "    ('drop_cols',FunctionTransformer(lambda x: x.drop(drop_cols_lgb, axis=1))),\n",
    "    (\"algo\",LGBMRegressor(n_jobs = -1,colsample_bytree = 0.8,learning_rate = 0.14166,max_depth = 20,num_leaves = 144,\n",
    "                           reg_alpha = 10,reg_lambda = 2.8426,subsample = 0.229))\n",
    "])\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('drop_cols',FunctionTransformer(lambda x: x.drop(drop_cols_xgb, axis=1))),\n",
    "    (\"algo\",XGBRegressor(colsample_bytree = 0.5655,gamma = 2,learning_rate = 0.0434,max_depth = 10,n_estimators = 196,\n",
    "                         reg_alpha = 3.795386101923234,reg_lambda = 0.001,subsample = 0.4933248871865059,random_state = 42,n_jobs = -1,tree_method = \"hist\"))\n",
    "]) \n",
    "\n",
    "estimators = [\n",
    "    (\"CatB\", CatBoostRegressor(iterations = 300,silent = True,colsample_bylevel = 0.81959, max_depth = 8)),\n",
    "    (\"LGBM\", LGBMRegressor(random_state = 42,n_jobs = -1,colsample_bytree = 0.5526,learning_rate = 0.08664,max_depth = 10,num_leaves = 155,\n",
    "                           reg_alpha = 0.1458,reg_lambda = 1.38465,subsample = 0.72301)),\n",
    "    (\"XGB\", XGBRegressor(colsample_bytree = 0.64846,gamma = 9,learning_rate = 0.0425,max_depth = 10,n_estimators = 167,reg_alpha = 0.0165,\n",
    "                         reg_lambda = 0.07,subsample = 0.44,random_state = 42,n_jobs = -1,tree_method = \"hist\")),\n",
    "    (\"CatB2\", pipeline_catb),\n",
    "    (\"LGBM2\", pipeline_lgb),\n",
    "    (\"XGB2\", pipeline_xgb)\n",
    "]\n",
    "\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator= LGBMRegressor(random_state = 42)\n",
    ")\n",
    "\n",
    "reg_xgb = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator= XGBRegressor(random_state = 42,tree_method = \"hist\")\n",
    ")\n",
    "\n",
    "voting = VotingRegressor(\n",
    "    estimators = estimators\n",
    ")\n",
    "\n",
    "list_model = {\n",
    "    'stack1' : reg,\n",
    "    \"stack2\" : reg_xgb,\n",
    "    \"voting\" : voting\n",
    "}\n",
    "\n",
    "df_model_stack = train_model_ts(list_model,X_train,y_train,X_test,y_test,mean_squared_error,cv,\n",
    "                        \"neg_root_mean_squared_error\",pipeline_base,groups)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['prediction'] = df_model_stack.model[0].predict(df_test)\n",
    "df_sub = df_test[['property_key','dataseries','prediction']]\n",
    "df_sub.columns = ['property_key','contractDate','prediction']\n",
    "df_sub.to_csv(\"voting3.csv\",index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Supervised",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
